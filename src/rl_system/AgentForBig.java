package rl_system;

import java.io.IOException;
import java.util.HashMap;

import algorithm.Algorithm;
import algorithm.EI_usm.EI_USM;
import algorithm.usm.USM;
import environment.Environment;
import environment.maze.Maze;

public class AgentForBig {
	public Algorithm algo;
	public Environment envir;
	public final int MAX_TIME = 20000; //MAX_TIME是一次所走的步E盗俊
	public int courseNum = 0;
	public int MaxNum = 8;
	public String text = "";

	public int stateNum; // agent有理由知道算法给出了多少个状态，即使实际上并没有用处

	public HashMap<String, Integer> actionList;
	public HashMap<String, Integer> observationList;

	public AgentForBig() throws IOException {
		this.actionList = new HashMap<String, Integer>(); //幼骷合。String是幼鞯拿Q，Integer是代。
		this.observationList = new HashMap<String, Integer>(); //^察集合。Sting是^察的名Q，Integer是代。
		this.agentInit();
	}

	/**
	 * agent初始化方法，之后会增加参数
	 * 
	 * @throws IOException
	 */
	private void agentInit() throws IOException {
		// 选取环境,初始化A和O集
		this.envir = new Maze("GridMaze64.txt"); //建出h境。@e可以xh境。
		// this.envir = new Maze("Hallway2.txt");
		// this.envir = new Maze(6, 6, -0.1, 200, MGF.PRIM);
		// this.envir.outputMaze("output.txt");
		this.actionList = envir.getActionList(); //@取h境定的幼骷。
		this.observationList = envir.getObservationList(); //@取h境定的^察集合。一共有16N^察，四面Φ那r。
		// 选取算法

		// this.algo = new KSIP_USM(observationList.size(), actionList.size());
		this.algo = new USM(observationList.size(), actionList.size()); //建一USM算法。
		//this.algo = new EI_USM(observationList.size(), actionList.size());
		return;
	}

	/**
	 * 决策方法，选择下一个动作
	 * @emakeStep的意思是做一次完整的。
	 */
	public void makeStep() {
		long p = System.currentTimeMillis();
		int actionIndex;
		int t = 0; //t是r器。

		// 环境接收动作
		int newO = this.envir.newStart(); // newO是新的_始的起c的^察值。
		this.algo.newStart(newO); //newStart把newO入到算法中去,告知USM@newO是一新的_始。

		while (t < MAX_TIME) {
			actionIndex = algo.makeDecision(); //USM中@取下一步的Q策actionIndex。
			// System.out.print(t + ":");
			if (envir.execute(actionIndex)) { //在h境中绦Q策actionIndex。如果撞Γt返回false，如果移映晒Γt返回true。如果actionIndex=0，即停留不樱也是返回true。
				algo.generateInstance(actionIndex, envir.getLastO(), envir.getLastR()); //oUSM入一例。
				if (!envir.isGoal()) { //h境，代理是否到_了Kc？
					// 算法产生实例并统计
					stateNum = algo.getStateNum(); //如果]有_了Kc，tUSM中@取B的盗浚更新stateNum的值。
				} else {
					// 抵达目标，准备下一次学习
					// System.out.println("One course finished!");
					// this.algo.printQvalueTable();
					courseNum++;//courseNum的是趟怠
					// long n = System.currentTimeMillis();
					// System.out.println(courseNum + " times training");
					// System.out.println(t + " steps cost " + String.valueOf(n - p) + " millis");
					// System.out.println("ADR: " + this.algo.getADR());
					newO = this.envir.newStart(); //// newO是新的_始的起c的^察值。
					this.algo.newStart(newO);//newStart把newO入到算法中去,告知USM@newO是一新的_始。
				}
				t++;//r器自增。
				if (t == 500 || t == 1000 || t == 1500 || t == 2000 || t == 2500 || t == 3000 || t == 4000 || t == 5000
						|| t == 6000 || t == 8000 || t == 10000 || t == 15000 || t == 20000) {
					//在一些特殊的rgc，@取USM的ADR值。
					long n = System.currentTimeMillis();
					text = text.concat(String.valueOf(this.algo.getADR()) + '\t');
					text = text.concat(String.valueOf(n - p) + '\n'); //系yrg。
				}
			}
		}
		System.out.println(text);
	}

	/**
	 * 结束方法，会将本次学习的记录导出
	 * 在makeStep 之後，\行此方法@取ADR。
	 *
	 */
	public void generateADR() {
		// this.algo.printQvalueTable();
		System.out.println("ADR: " + this.algo.getADR());
		System.out.println();
	}
}
